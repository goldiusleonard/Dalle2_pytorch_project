{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Import Libraries</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\.virtualenvs\\proj-SA3AAx3w\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "from dalle2_pytorch import DALLE2, DiffusionPriorNetwork, DiffusionPrior, Unet, Decoder, OpenAIClipAdapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Data Preprocessing</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 72/32801 [00:23<2:58:28,  3.06it/s]"
     ]
    }
   ],
   "source": [
    "# Image Transformations\n",
    "\n",
    "# Change your input size here (must be square like 224 x 224)\n",
    "input_size = (256, 256)\n",
    "\n",
    "# Change your text embedding size here (default setting is the same with input size)\n",
    "text_embedding_size = 256\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize(input_size),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "       mean=[0.485, 0.456, 0.406],\n",
    "       std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Open json file\n",
    "f = open(\"./annotations/stuff_train2017.json\", \"r\")\n",
    "json_file = json.load(f)\n",
    "f.close()\n",
    "\n",
    "categories = json_file['categories']\n",
    "raw_annot = json_file['annotations']\n",
    "\n",
    "word_to_ix = {}\n",
    "\n",
    "for idx, category in enumerate(categories):\n",
    "    word_to_ix[category['name']] = idx\n",
    "\n",
    "num_categories = len(categories)\n",
    "embeds = nn.Embedding(num_categories, text_embedding_size)\n",
    "\n",
    "# Image folder path\n",
    "train_path = Path(\"./train2017/\")\n",
    "\n",
    "category_index_list = []\n",
    "images_list = []\n",
    "\n",
    "for annot in tqdm(raw_annot):\n",
    "    annot_cat = categories[int(annot['category_id']) - num_categories]['name']\n",
    "    lookup_tensor = torch.tensor([word_to_ix[annot_cat]], dtype=torch.int)\n",
    "    annot_embed = embeds(lookup_tensor)\n",
    "    category_index_list.append(annot_embed.type(torch.LongTensor))\n",
    "\n",
    "    bbox = (int(annot['bbox'][0]), int(annot['bbox'][1]), int(annot['bbox'][2]), int(annot['bbox'][3]))\n",
    "\n",
    "    patt = \"*\" + str(annot['image_id']) + \".jpg\"\n",
    "\n",
    "    for img_path in train_path.glob(patt):\n",
    "        basename = os.path.basename(img_path)\n",
    "        basename = basename.lstrip('0')\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = img.crop(bbox)\n",
    "        img_tensor = transform(img)\n",
    "        img_tensor = img_tensor.type(torch.FloatTensor)\n",
    "        images_list.append(img_tensor.unsqueeze(0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Normalize the Text Embedding to minimum = 0</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_raw = torch.cat(category_index_list, dim=1)\n",
    "minimum_text_embedding = torch.min(text_raw)\n",
    "\n",
    "new_category_index_list = []\n",
    "\n",
    "if minimum_text_embedding < 0:\n",
    "    for category_index in category_index_list:\n",
    "        new_category_index_list.append(category_index + torch.abs(minimum_text_embedding))\n",
    "else:\n",
    "    new_category_index_list = category_index_list.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Run Batch Training</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4101/4101 [1:07:04<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# openai pretrained clip - defaults to ViT/B-32\n",
    "clip = OpenAIClipAdapter()\n",
    "\n",
    "size = len(new_category_index_list)\n",
    "\n",
    "# Change your batch size here\n",
    "batch_size = 32\n",
    "\n",
    "prior_network = DiffusionPriorNetwork(\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    dim_head = 64,\n",
    "    heads = 8\n",
    ").cuda()\n",
    "\n",
    "diffusion_prior = DiffusionPrior(\n",
    "    net = prior_network,\n",
    "    clip = clip,\n",
    "    timesteps = 100,\n",
    "    cond_drop_prob = 0.2\n",
    ").cuda()\n",
    "\n",
    "unet1 = Unet(\n",
    "    dim = 128,\n",
    "    image_embed_dim = 512,\n",
    "    cond_dim = 128,\n",
    "    channels = 3,\n",
    "    dim_mults=(1, 2, 4, 8)\n",
    ").cuda()\n",
    "\n",
    "unet2 = Unet(\n",
    "    dim = 16,\n",
    "    image_embed_dim = 512,\n",
    "    cond_dim = 128,\n",
    "    channels = 3,\n",
    "    dim_mults = (1, 2, 4, 8, 16)\n",
    ").cuda()\n",
    "\n",
    "decoder = Decoder(\n",
    "    unet = (unet1, unet2),\n",
    "    image_sizes = (128, 256),\n",
    "    clip = clip,\n",
    "    timesteps = 100,\n",
    "    image_cond_drop_prob = 0.1,\n",
    "    text_cond_drop_prob = 0.5,\n",
    "    condition_on_text_encodings = False  # set this to True if you wish to condition on text during training and sampling\n",
    ").cuda()\n",
    "\n",
    "idx_list = range(0, size, batch_size)\n",
    "\n",
    "for idx in tqdm(idx_list):\n",
    "    if (idx + batch_size) > (size - 1):\n",
    "        text = torch.cat(new_category_index_list[idx:], dim=0).cuda()\n",
    "        images = torch.cat(images_list[idx:], dim=0).cuda()\n",
    "    else:\n",
    "        text = torch.cat(new_category_index_list[idx:idx+batch_size], dim=0).cuda()\n",
    "        images = torch.cat(images_list[idx:idx+batch_size], dim=0).cuda()\n",
    "    \n",
    "    loss = diffusion_prior(text, images)\n",
    "    loss.backward()\n",
    "\n",
    "    for unet_number in (1, 2):\n",
    "        loss = decoder(images, unet_number = unet_number) # this can optionally be decoder(images, text) if you wish to condition on the text encodings as well, though it was hinted in the paper it didn't do much\n",
    "        loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 100/100 [00:01<00:00, 74.25it/s]\n",
      "sampling loop time step: 100%|██████████| 100/100 [00:23<00:00,  4.27it/s]\n",
      "sampling loop time step: 100%|██████████| 100/100 [00:10<00:00,  9.66it/s]\n",
      "2it [00:37, 18.87s/it]\n"
     ]
    }
   ],
   "source": [
    "dalle2 = DALLE2(\n",
    "    prior = diffusion_prior,\n",
    "    decoder = decoder\n",
    ").cuda()\n",
    "\n",
    "# Change the model weight save path here (end with \".pt\")\n",
    "save_path = \"./dalle2.pt\"\n",
    "\n",
    "# Change the test result image save path (should be a directory or folder)\n",
    "test_img_save_path = \"./result\"\n",
    "\n",
    "if not os.path.exists(test_img_save_path):\n",
    "    os.makedirs(test_img_save_path)\n",
    "\n",
    "torch.save(dalle2.state_dict(), save_path)\n",
    "\n",
    "test_imgs_tensor = dalle2(\n",
    "    ['blanket', 'branch'], # text input for the model (can be more than one)\n",
    "    cond_scale = 2. # classifier free guidance strength (> 1 would strengthen the condition)\n",
    ")\n",
    "\n",
    "for test_idx, test_img_tensor in enumerate(test_imgs_tensor):\n",
    "    test_img = test_img_tensor.cpu().numpy().reshape(3, input_size[0], input_size[1])\n",
    "    \n",
    "    new_test_img = np.zeros([input_size[0], input_size[1], 3])\n",
    "    new_test_img[:,:,0] = test_img[0]\n",
    "    new_test_img[:,:,1] = test_img[1]\n",
    "    new_test_img[:,:,2] = test_img[2]\n",
    "\n",
    "    test_save_path = os.path.join(test_img_save_path, f\"test_{test_idx}.jpg\")\n",
    "\n",
    "    cv2.imwrite(test_save_path, new_test_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Load DALLE2 Model (Use if you want to get the trained model)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dalle2 = DALLE2(\n",
    "    prior = diffusion_prior,\n",
    "    decoder = decoder\n",
    ").cuda()\n",
    "\n",
    "# Change your model path (\".pt\" file)\n",
    "load_model_path = \"./dalle2.pt\"\n",
    "\n",
    "dalle2.load_state_dict(torch.load(load_model_path))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
