{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Import Libraries</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "from dalle2_pytorch import DALLE2, DiffusionPriorNetwork, DiffusionPrior, Unet, Decoder, OpenAIClipAdapter\n",
    "from torchvision.datasets.coco import CocoCaptions\n",
    "from IPython.display import clear_output\n",
    "import clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Setting Dataset & Training Parameters</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change your input size here\n",
    "input_image_size = 512\n",
    "\n",
    "# Change your text embedding size here (default setting is the same with input size)\n",
    "text_embedding_size = 512\n",
    "\n",
    "# Change your batch size here\n",
    "batch_size = 1\n",
    "\n",
    "# Change your epoch here\n",
    "epoch = 1\n",
    "\n",
    "# Change your train image root path here\n",
    "train_img_path = \"./train2014/\"\n",
    "\n",
    "# Change your train annot json path here\n",
    "train_annot_path = \"./annotations/captions_train2014.json\"\n",
    "\n",
    "# Change your device (\"cpu\" or \"cuda\")\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Data Preprocessing</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.93s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize((input_image_size, input_image_size)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "       mean=[0.485, 0.456, 0.406],\n",
    "       std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "train_data = CocoCaptions(\n",
    "    root=train_img_path,\n",
    "    annFile=train_annot_path,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Create Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai pretrained clip - defaults to ViT/B-32\n",
    "OpenAIClip = OpenAIClipAdapter()\n",
    "\n",
    "prior_network = DiffusionPriorNetwork(\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    dim_head = 64,\n",
    "    heads = 8\n",
    ")\n",
    "\n",
    "diffusion_prior = DiffusionPrior(\n",
    "    net = prior_network,\n",
    "    clip = OpenAIClip,\n",
    "    timesteps = 100,\n",
    "    cond_drop_prob = 0.2\n",
    ").to(device)\n",
    "\n",
    "# diffusion_prior = nn.DataParallel(diffusion_prior)\n",
    "\n",
    "unet1 = Unet(\n",
    "    dim = 128,\n",
    "    image_embed_dim = 512,\n",
    "    cond_dim = 128,\n",
    "    channels = 3,\n",
    "    dim_mults=(1, 2, 4, 8)\n",
    ")\n",
    "\n",
    "unet2 = Unet(\n",
    "    dim = 16,\n",
    "    image_embed_dim = 512,\n",
    "    cond_dim = 128,\n",
    "    channels = 3,\n",
    "    dim_mults = (1, 2, 4, 8, 16)\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    unet = (unet1, unet2),\n",
    "    image_sizes = (256, 512),\n",
    "    clip = OpenAIClip,\n",
    "    timesteps = 100,\n",
    "    image_cond_drop_prob = 0.1,\n",
    "    text_cond_drop_prob = 0.5,\n",
    "    condition_on_text_encodings = False  # set this to True if you wish to condition on text during training and sampling\n",
    ").to(device)\n",
    "\n",
    "# decoder = nn.DataParallel(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Run training</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run training ...\n",
      "Epoch 1 / 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 208/82783 [09:24<61:52:06,  2.70s/it]"
     ]
    }
   ],
   "source": [
    "curr_size = 1\n",
    "train_size = len(train_data)\n",
    "idx_list = range(0, train_size, batch_size)\n",
    "\n",
    "model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "for curr_epoch in range(epoch):\n",
    "    print(\"Run training ...\")\n",
    "    print(f\"Epoch {curr_epoch+1} / {epoch}\")\n",
    "    \n",
    "    for batch_idx in tqdm(idx_list):\n",
    "        if (batch_idx + batch_size) > train_size - 1:\n",
    "            iter_idx = range(batch_size, train_size, 1)\n",
    "        else:\n",
    "            iter_idx = range(batch_idx, batch_idx+batch_size, 1)\n",
    "\n",
    "        image_list = []\n",
    "        text_list = []\n",
    "        \n",
    "        for curr_idx in iter_idx:\n",
    "            image, target = train_data[curr_idx]\n",
    "            image = image.unsqueeze(0).to(device)\n",
    "            text = clip.tokenize(target).to(device)\n",
    "            if device == \"cuda\":\n",
    "                text = model.encode_text(text).type(torch.cuda.LongTensor)\n",
    "            else:\n",
    "                text = model.encode_text(text).type(torch.LongTensor)\n",
    "\n",
    "            text_size = len(text)\n",
    "            for i in range(text_size):\n",
    "                image_list.append(image)\n",
    "            \n",
    "            text_list.append(text)\n",
    "\n",
    "\n",
    "        text = torch.cat(text_list, dim=0)\n",
    "        minimum_text_embedding = torch.min(text)\n",
    "\n",
    "        new_text_list = []\n",
    "\n",
    "        if minimum_text_embedding < 0:\n",
    "            for text_embed in text_list:\n",
    "                new_text_list.append(text_embed + torch.abs(minimum_text_embedding))\n",
    "        else:\n",
    "            new_text_list = text_list.copy()\n",
    "            \n",
    "        text = torch.cat(new_text_list, dim=0).to(device)\n",
    "        image = torch.cat(image_list, dim=0).to(device)\n",
    "    \n",
    "        loss = diffusion_prior(text, image)\n",
    "        loss.backward()\n",
    "\n",
    "        for unet_number in (1, 2):\n",
    "            loss = decoder(image, unet_number = unet_number) # this can optionally be decoder(images, text) if you wish to condition on the text encodings as well, though it was hinted in the paper it didn't do much\n",
    "            loss.backward()\n",
    "\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dalle2 = DALLE2(\n",
    "    prior = diffusion_prior,\n",
    "    decoder = decoder\n",
    ").to(device)\n",
    "\n",
    "# Change the model weight save path here (end with \".pt\")\n",
    "save_path = \"./dalle2.pt\"\n",
    "\n",
    "# Change the test result image save path (should be a directory or folder)\n",
    "test_img_save_path = \"./result\"\n",
    "\n",
    "if not os.path.exists(test_img_save_path):\n",
    "    os.makedirs(test_img_save_path)\n",
    "\n",
    "torch.save(dalle2.state_dict(), save_path)\n",
    "\n",
    "test_imgs_tensor = dalle2(\n",
    "    ['glistening morning dew on a flower petal'], # text input for the model (can be more than one)\n",
    "    cond_scale = 2. # classifier free guidance strength (> 1 would strengthen the condition)\n",
    ")\n",
    "\n",
    "for test_idx, test_img_tensor in enumerate(test_imgs_tensor):\n",
    "    test_img = test_img_tensor.cpu().numpy().reshape(3, input_image_size, input_image_size)\n",
    "    \n",
    "    new_test_img = np.zeros([input_image_size, input_image_size, 3])\n",
    "    new_test_img[:,:,0] = test_img[0]\n",
    "    new_test_img[:,:,1] = test_img[1]\n",
    "    new_test_img[:,:,2] = test_img[2]\n",
    "\n",
    "    print(new_test_img.shape)\n",
    "\n",
    "    test_save_path = os.path.join(test_img_save_path, f\"test_{test_idx}.jpg\")\n",
    "\n",
    "    cv2.imwrite(test_save_path, new_test_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Load DALLE2 Model (Use if you want to get the trained model)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dalle2 = DALLE2(\n",
    "#     prior = diffusion_prior,\n",
    "#     decoder = decoder\n",
    "# ).cuda()\n",
    "\n",
    "# # Change your model path (\".pt\" file)\n",
    "# load_model_path = \"./dalle2.pt\"\n",
    "\n",
    "# dalle2.load_state_dict(torch.load(load_model_path))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
