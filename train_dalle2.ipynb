{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Import Libraries</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "from dalle2_pytorch import DALLE2, DiffusionPriorNetwork, DiffusionPrior, Unet, Decoder, OpenAIClipAdapter\n",
    "from torchvision.datasets.coco import CocoCaptions\n",
    "from IPython.display import clear_output\n",
    "import clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Setting Dataset & Training Parameters</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change your input size here\n",
    "input_image_size = 128\n",
    "\n",
    "# Change your max text embedding value here\n",
    "max_text_embedding_val = 1000\n",
    "\n",
    "# Change your batch size here\n",
    "batch_size = 1\n",
    "\n",
    "# Change your epoch here\n",
    "epoch = 1\n",
    "\n",
    "# Change your train image root path here\n",
    "train_img_path = \"./train2014/\"\n",
    "\n",
    "# Change your train annot json path here\n",
    "train_annot_path = \"./annotations/captions_train2014.json\"\n",
    "\n",
    "# Change your device (\"cpu\" or \"cuda\")\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Data Preprocessing</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.93s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize((input_image_size, input_image_size)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "       mean=[0.485, 0.456, 0.406],\n",
    "       std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "train_data = CocoCaptions(\n",
    "    root=train_img_path,\n",
    "    annFile=train_annot_path,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Create Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai pretrained clip - defaults to ViT/B-32\n",
    "OpenAIClip = OpenAIClipAdapter()\n",
    "\n",
    "prior_network = DiffusionPriorNetwork(\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    dim_head = 64,\n",
    "    heads = 8\n",
    ")\n",
    "\n",
    "diffusion_prior = DiffusionPrior(\n",
    "    net = prior_network,\n",
    "    clip = OpenAIClip,\n",
    "    timesteps = 100,\n",
    "    cond_drop_prob = 0.2\n",
    ").to(device)\n",
    "\n",
    "# diffusion_prior = nn.DataParallel(diffusion_prior)\n",
    "\n",
    "unet1 = Unet(\n",
    "    dim = 128,\n",
    "    image_embed_dim = 512,\n",
    "    cond_dim = 128,\n",
    "    channels = 3,\n",
    "    dim_mults=(1, 2, 4, 8)\n",
    ")\n",
    "\n",
    "unet2 = Unet(\n",
    "    dim = 16,\n",
    "    image_embed_dim = 256,\n",
    "    cond_dim = 128,\n",
    "    channels = 3,\n",
    "    dim_mults = (1, 2, 4, 8, 16)\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    unet = (unet1, unet2),\n",
    "    image_sizes = (128, 256),\n",
    "    clip = OpenAIClip,\n",
    "    timesteps = 100,\n",
    "    image_cond_drop_prob = 0.1,\n",
    "    text_cond_drop_prob = 0.5,\n",
    "    condition_on_text_encodings = True  # set this to True if you wish to condition on text during training and sampling\n",
    ").to(device)\n",
    "\n",
    "# decoder = nn.DataParallel(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Run training</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run training ...\n",
      "Epoch 1 / 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 1281/82783 [16:10<17:16:21,  1.31it/s]"
     ]
    }
   ],
   "source": [
    "train_size = len(train_data)\n",
    "idx_list = range(0, train_size, batch_size)\n",
    "\n",
    "model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "for curr_epoch in range(epoch):\n",
    "    print(\"Run training ...\")\n",
    "    print(f\"Epoch {curr_epoch+1} / {epoch}\")\n",
    "    \n",
    "    for batch_idx in tqdm(idx_list):\n",
    "        if (batch_idx + batch_size) > train_size - 1:\n",
    "            iter_idx = range(batch_idx, train_size, 1)\n",
    "        else:\n",
    "            iter_idx = range(batch_idx, batch_idx+batch_size, 1)\n",
    "\n",
    "        image_list = []\n",
    "        text_list = []\n",
    "        \n",
    "        for curr_idx in iter_idx:\n",
    "            image, target = train_data[curr_idx]\n",
    "            image = image.unsqueeze(0).to(device)\n",
    "            text = clip.tokenize(target).to(device)\n",
    "            text = model.encode_text(text).to(device)\n",
    "\n",
    "            text_size = len(text)\n",
    "            for i in range(text_size):\n",
    "                image_list.append(image)\n",
    "            \n",
    "            text_list.append(text)\n",
    "\n",
    "        text = torch.cat(text_list, dim=0)\n",
    "\n",
    "        new_text_list = []\n",
    "\n",
    "        for text_embed in text_list:\n",
    "            text_embed -= text_embed.min(1, keepdim=True)[0]\n",
    "            text_embed /= text_embed.max(1, keepdim=True)[0]\n",
    "            text_embed *= max_text_embedding_val\n",
    "\n",
    "            new_text_list.append(text_embed.type(torch.LongTensor).to(device))\n",
    "            \n",
    "        text = torch.cat(new_text_list, dim=0).to(device)\n",
    "        image = torch.cat(image_list, dim=0).to(device)\n",
    "    \n",
    "        loss = diffusion_prior(text, image)\n",
    "        loss.backward()\n",
    "\n",
    "        for unet_number in (1, 2):\n",
    "            loss = decoder(image, text, unet_number = unet_number) # this can optionally be decoder(images, text) if you wish to condition on the text encodings as well, though it was hinted in the paper it didn't do much\n",
    "            loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Administrator\\Desktop\\Dalle2_pytorch_project-main\\train_dalle2.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Dalle2_pytorch_project-main/train_dalle2.ipynb#ch0000014?line=11'>12</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(test_img_save_path):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Dalle2_pytorch_project-main/train_dalle2.ipynb#ch0000014?line=12'>13</a>\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(test_img_save_path)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Dalle2_pytorch_project-main/train_dalle2.ipynb#ch0000014?line=14'>15</a>\u001b[0m torch\u001b[39m.\u001b[39;49msave(dalle2\u001b[39m.\u001b[39;49mstate_dict(), save_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Dalle2_pytorch_project-main/train_dalle2.ipynb#ch0000014?line=16'>17</a>\u001b[0m test_input \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mcute puppy chasing after a squirrel\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m# text input for the model (can be more than one)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Dalle2_pytorch_project-main/train_dalle2.ipynb#ch0000014?line=18'>19</a>\u001b[0m test_imgs \u001b[39m=\u001b[39m dalle2(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Dalle2_pytorch_project-main/train_dalle2.ipynb#ch0000014?line=19'>20</a>\u001b[0m     test_input,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Dalle2_pytorch_project-main/train_dalle2.ipynb#ch0000014?line=20'>21</a>\u001b[0m     cond_scale \u001b[39m=\u001b[39m \u001b[39m2.\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Dalle2_pytorch_project-main/train_dalle2.ipynb#ch0000014?line=21'>22</a>\u001b[0m     return_pil_images \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m \u001b[39m# classifier free guidance strength (> 1 would strengthen the condition)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Dalle2_pytorch_project-main/train_dalle2.ipynb#ch0000014?line=22'>23</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\serialization.py:380\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/serialization.py?line=377'>378</a>\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/serialization.py?line=378'>379</a>\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/serialization.py?line=379'>380</a>\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/serialization.py?line=380'>381</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/serialization.py?line=381'>382</a>\u001b[0m _legacy_save(obj, opened_file, pickle_module, pickle_protocol)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\serialization.py:601\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/serialization.py?line=596'>597</a>\u001b[0m \u001b[39m# given that we copy things around anyway, we might use storage.cpu()\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/serialization.py?line=597'>598</a>\u001b[0m \u001b[39m# this means to that to get tensors serialized, you need to implement\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/serialization.py?line=598'>599</a>\u001b[0m \u001b[39m# .cpu() on the underlying Storage\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/serialization.py?line=599'>600</a>\u001b[0m \u001b[39mif\u001b[39;00m storage\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/serialization.py?line=600'>601</a>\u001b[0m     storage \u001b[39m=\u001b[39m storage\u001b[39m.\u001b[39;49mcpu()\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/serialization.py?line=601'>602</a>\u001b[0m \u001b[39m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/serialization.py?line=602'>603</a>\u001b[0m num_bytes \u001b[39m=\u001b[39m storage\u001b[39m.\u001b[39mnbytes()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\storage.py:83\u001b[0m, in \u001b[0;36m_StorageBase.cpu\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/storage.py?line=80'>81</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcpu\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/storage.py?line=81'>82</a>\u001b[0m     \u001b[39m\"\"\"Returns a CPU copy of this storage if it's not already on the CPU\"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/storage.py?line=82'>83</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _type(\u001b[39mself\u001b[39;49m, \u001b[39mgetattr\u001b[39;49m(torch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\_utils.py:45\u001b[0m, in \u001b[0;36m_type\u001b[1;34m(self, dtype, non_blocking, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/_utils.py?line=42'>43</a>\u001b[0m \u001b[39mif\u001b[39;00m dtype\u001b[39m.\u001b[39mis_sparse:\n\u001b[0;32m     <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/_utils.py?line=43'>44</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot cast dense tensor to sparse tensor\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/_utils.py?line=44'>45</a>\u001b[0m \u001b[39mreturn\u001b[39;00m dtype(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize())\u001b[39m.\u001b[39;49mcopy_(\u001b[39mself\u001b[39;49m, non_blocking)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "dalle2 = DALLE2(\n",
    "    prior = diffusion_prior,\n",
    "    decoder = decoder\n",
    ").to(device)\n",
    "\n",
    "# Change the model weight save path here (end with \".pt\")\n",
    "save_path = \"./dalle2.pt\"\n",
    "\n",
    "# Change the test result image save path (should be a directory or folder)\n",
    "test_img_save_path = \"./result\"\n",
    "\n",
    "if not os.path.exists(test_img_save_path):\n",
    "    os.makedirs(test_img_save_path)\n",
    "\n",
    "torch.save(dalle2.state_dict(), save_path)\n",
    "\n",
    "test_input = ['cute puppy chasing after a squirrel'] # text input for the model (can be more than one)\n",
    "\n",
    "test_imgs = dalle2(\n",
    "    test_input,\n",
    "    cond_scale = 2.,\n",
    "    return_pil_images = True # classifier free guidance strength (> 1 would strengthen the condition)\n",
    ")\n",
    "\n",
    "for test_idx, test_img in enumerate(test_imgs):\n",
    "    test_save_path = os.path.join(test_img_save_path, f\"{test_input[test_idx]}.jpg\")\n",
    "    test_img.save(Path(test_save_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Load DALLE2 Model (Use if you want to get the trained model)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\"\n",
    "\n",
    "# # openai pretrained clip - defaults to ViT/B-32\n",
    "# OpenAIClip = OpenAIClipAdapter()\n",
    "\n",
    "# prior_network = DiffusionPriorNetwork(\n",
    "#     dim = 512,\n",
    "#     depth = 6,\n",
    "#     dim_head = 64,\n",
    "#     heads = 8\n",
    "# )\n",
    "\n",
    "# diffusion_prior = DiffusionPrior(\n",
    "#     net = prior_network,\n",
    "#     clip = OpenAIClip,\n",
    "#     timesteps = 100,\n",
    "#     cond_drop_prob = 0.2\n",
    "# ).to(device)\n",
    "\n",
    "# # diffusion_prior = nn.DataParallel(diffusion_prior)\n",
    "\n",
    "# unet1 = Unet(\n",
    "#     dim = 128,\n",
    "#     image_embed_dim = 512,\n",
    "#     cond_dim = 128,\n",
    "#     channels = 3,\n",
    "#     dim_mults=(1, 2, 4, 8)\n",
    "# )\n",
    "\n",
    "# unet2 = Unet(\n",
    "#     dim = 16,\n",
    "#     image_embed_dim = 256,\n",
    "#     cond_dim = 128,\n",
    "#     channels = 3,\n",
    "#     dim_mults = (1, 2, 4, 8, 16)\n",
    "# )\n",
    "\n",
    "# decoder = Decoder(\n",
    "#     unet = (unet1, unet2),\n",
    "#     image_sizes = (128, 256),\n",
    "#     clip = OpenAIClip,\n",
    "#     timesteps = 100,\n",
    "#     image_cond_drop_prob = 0.1,\n",
    "#     text_cond_drop_prob = 0.5,\n",
    "#     condition_on_text_encodings = True  # set this to True if you wish to condition on text during training and sampling\n",
    "# ).to(device)\n",
    "\n",
    "# dalle2 = DALLE2(\n",
    "#     prior = diffusion_prior,\n",
    "#     decoder = decoder\n",
    "# ).cuda()\n",
    "\n",
    "# # Change your model path (\".pt\" file)\n",
    "# load_model_path = \"./dalle2.pt\"\n",
    "\n",
    "# dalle2.load_state_dict(torch.load(load_model_path))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
