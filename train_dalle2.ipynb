{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Import Libraries</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "from dalle2_pytorch import DALLE2, DiffusionPriorNetwork, DiffusionPrior, Unet, Decoder, OpenAIClipAdapter\n",
    "from torchvision.datasets.coco import CocoCaptions\n",
    "from IPython.display import clear_output\n",
    "import clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Setting Dataset & Training Parameters</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change your input size here\n",
    "input_image_size = 256\n",
    "\n",
    "# Change your text embedding size here (max: 512)\n",
    "text_embedding_size = 512\n",
    "\n",
    "# Change your batch size here\n",
    "batch_size = 1\n",
    "\n",
    "# Change your epoch here\n",
    "epoch = 1\n",
    "\n",
    "# Change your train image root path here\n",
    "train_img_path = \"./train2014/\"\n",
    "\n",
    "# Change your train annot json path here\n",
    "train_annot_path = \"./annotations/captions_train2014.json\"\n",
    "\n",
    "# Change your device (\"cpu\" or \"cuda\")\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Data Preprocessing</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.95s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize((input_image_size, input_image_size)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "       mean=[0.485, 0.456, 0.406],\n",
    "       std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "train_data = CocoCaptions(\n",
    "    root=train_img_path,\n",
    "    annFile=train_annot_path,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Create Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai pretrained clip - defaults to ViT/B-32\n",
    "OpenAIClip = OpenAIClipAdapter()\n",
    "\n",
    "prior_network = DiffusionPriorNetwork(\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    dim_head = 64,\n",
    "    heads = 8\n",
    ")\n",
    "\n",
    "diffusion_prior = DiffusionPrior(\n",
    "    net = prior_network,\n",
    "    clip = OpenAIClip,\n",
    "    timesteps = 100,\n",
    "    cond_drop_prob = 0.2\n",
    ").to(device)\n",
    "\n",
    "# diffusion_prior = nn.DataParallel(diffusion_prior)\n",
    "\n",
    "unet1 = Unet(\n",
    "    dim = 128,\n",
    "    image_embed_dim = 512,\n",
    "    cond_dim = 128,\n",
    "    channels = 3,\n",
    "    dim_mults=(1, 2, 4, 8)\n",
    ")\n",
    "\n",
    "unet2 = Unet(\n",
    "    dim = 16,\n",
    "    image_embed_dim = 256,\n",
    "    cond_dim = 128,\n",
    "    channels = 3,\n",
    "    dim_mults = (1, 2, 4, 8, 16)\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    unet = (unet1, unet2),\n",
    "    image_sizes = (128, 256),\n",
    "    clip = OpenAIClip,\n",
    "    timesteps = 100,\n",
    "    image_cond_drop_prob = 0.1,\n",
    "    text_cond_drop_prob = 0.5,\n",
    "    condition_on_text_encodings = False  # set this to True if you wish to condition on text during training and sampling\n",
    ").to(device)\n",
    "\n",
    "# decoder = nn.DataParallel(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Run training</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run training ...\n",
      "Epoch 1 / 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 82782/82783 [16:23:52<00:00,  1.40it/s]  \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 15.88 GiB total capacity; 11.67 GiB already allocated; 4.00 MiB free; 14.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Administrator\\Desktop\\Dalle2_pytorch_project-main\\train_dalle2.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Dalle2_pytorch_project-main/train_dalle2.ipynb#ch0000009?line=22'>23</a>\u001b[0m text \u001b[39m=\u001b[39m clip\u001b[39m.\u001b[39mtokenize(target)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Dalle2_pytorch_project-main/train_dalle2.ipynb#ch0000009?line=23'>24</a>\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Dalle2_pytorch_project-main/train_dalle2.ipynb#ch0000009?line=24'>25</a>\u001b[0m     text \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode_text(text)\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mLongTensor)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Dalle2_pytorch_project-main/train_dalle2.ipynb#ch0000009?line=25'>26</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Dalle2_pytorch_project-main/train_dalle2.ipynb#ch0000009?line=26'>27</a>\u001b[0m     text \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode_text(text)\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mLongTensor)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\clip\\model.py:349\u001b[0m, in \u001b[0;36mCLIP.encode_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/clip/model.py?line=346'>347</a>\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_embedding\u001b[39m.\u001b[39mtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/clip/model.py?line=347'>348</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# NLD -> LND\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/clip/model.py?line=348'>349</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x)\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/clip/model.py?line=349'>350</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# LND -> NLD\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/clip/model.py?line=350'>351</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_final(x)\u001b[39m.\u001b[39mtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\clip\\model.py:204\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/clip/model.py?line=202'>203</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/clip/model.py?line=203'>204</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresblocks(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/container.py?line=140'>141</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/container.py?line=141'>142</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\clip\\model.py:192\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/clip/model.py?line=189'>190</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/clip/model.py?line=190'>191</a>\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(x))\n\u001b[1;32m--> <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/clip/model.py?line=191'>192</a>\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_2(x))\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/clip/model.py?line=192'>193</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\clip\\model.py:163\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/clip/model.py?line=160'>161</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/clip/model.py?line=161'>162</a>\u001b[0m     orig_type \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mdtype\n\u001b[1;32m--> <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/clip/model.py?line=162'>163</a>\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(x\u001b[39m.\u001b[39;49mtype(torch\u001b[39m.\u001b[39;49mfloat32))\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/clip/model.py?line=163'>164</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\u001b[39m.\u001b[39mtype(orig_type)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\normalization.py:189\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/normalization.py?line=187'>188</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/normalization.py?line=188'>189</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlayer_norm(\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/modules/normalization.py?line=189'>190</a>\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalized_shape, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\functional.py:2486\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/functional.py?line=2481'>2482</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/functional.py?line=2482'>2483</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/functional.py?line=2483'>2484</a>\u001b[0m         layer_norm, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, normalized_shape, weight\u001b[39m=\u001b[39mweight, bias\u001b[39m=\u001b[39mbias, eps\u001b[39m=\u001b[39meps\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/functional.py?line=2484'>2485</a>\u001b[0m     )\n\u001b[1;32m-> <a href='file:///c%3A/Users/Administrator/AppData/Local/Programs/Python/Python38/lib/site-packages/torch/nn/functional.py?line=2485'>2486</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mlayer_norm(\u001b[39minput\u001b[39;49m, normalized_shape, weight, bias, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 15.88 GiB total capacity; 11.67 GiB already allocated; 4.00 MiB free; 14.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "curr_size = 1\n",
    "train_size = len(train_data)\n",
    "idx_list = range(0, train_size, batch_size)\n",
    "\n",
    "model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "for curr_epoch in range(epoch):\n",
    "    print(\"Run training ...\")\n",
    "    print(f\"Epoch {curr_epoch+1} / {epoch}\")\n",
    "    \n",
    "    for batch_idx in tqdm(idx_list):\n",
    "        if (batch_idx + batch_size) > train_size - 1:\n",
    "            iter_idx = range(batch_idx, train_size, 1)\n",
    "        else:\n",
    "            iter_idx = range(batch_idx, batch_idx+batch_size, 1)\n",
    "\n",
    "        image_list = []\n",
    "        text_list = []\n",
    "        \n",
    "        for curr_idx in iter_idx:\n",
    "            image, target = train_data[curr_idx]\n",
    "            image = image.unsqueeze(0).to(device)\n",
    "            text = clip.tokenize(target).to(device)\n",
    "            if device == \"cuda\":\n",
    "                text = model.encode_text(text).type(torch.cuda.LongTensor)\n",
    "            else:\n",
    "                text = model.encode_text(text).type(torch.LongTensor)\n",
    "\n",
    "            text_size = len(text)\n",
    "            for i in range(text_size):\n",
    "                image_list.append(image)\n",
    "            \n",
    "            text_list.append(text[:text_embedding_size])\n",
    "\n",
    "        text = torch.cat(text_list, dim=0)\n",
    "        minimum_text_embedding = torch.min(text)\n",
    "\n",
    "        new_text_list = []\n",
    "\n",
    "        if minimum_text_embedding < 0:\n",
    "            for text_embed in text_list:\n",
    "                new_text_list.append(text_embed + torch.abs(minimum_text_embedding))\n",
    "        else:\n",
    "            new_text_list = text_list.copy()\n",
    "            \n",
    "        text = torch.cat(new_text_list, dim=0).to(device)\n",
    "        image = torch.cat(image_list, dim=0).to(device)\n",
    "    \n",
    "        loss = diffusion_prior(text, image)\n",
    "        loss.backward()\n",
    "\n",
    "        for unet_number in (1, 2):\n",
    "            loss = decoder(image, unet_number = unet_number) # this can optionally be decoder(images, text) if you wish to condition on the text encodings as well, though it was hinted in the paper it didn't do much\n",
    "            loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 100/100 [00:01<00:00, 73.05it/s]\n",
      "sampling loop time step: 100%|██████████| 100/100 [00:06<00:00, 14.46it/s]\n",
      "sampling loop time step: 100%|██████████| 100/100 [00:07<00:00, 13.16it/s]\n",
      "2it [00:18,  9.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "dalle2 = DALLE2(\n",
    "    prior = diffusion_prior,\n",
    "    decoder = decoder\n",
    ").to(device)\n",
    "\n",
    "# Change the model weight save path here (end with \".pt\")\n",
    "save_path = \"./dalle2.pt\"\n",
    "\n",
    "# Change the test result image save path (should be a directory or folder)\n",
    "test_img_save_path = \"./result\"\n",
    "\n",
    "if not os.path.exists(test_img_save_path):\n",
    "    os.makedirs(test_img_save_path)\n",
    "\n",
    "torch.save(dalle2.state_dict(), save_path)\n",
    "\n",
    "test_input = ['cute puppy chasing after a squirrel'] # text input for the model (can be more than one)\n",
    "\n",
    "test_imgs_tensor = dalle2(\n",
    "    test_input,\n",
    "    cond_scale = 2. # classifier free guidance strength (> 1 would strengthen the condition)\n",
    ")\n",
    "\n",
    "transform = T.ToPILImage()\n",
    "\n",
    "for test_idx, test_img_tensor in enumerate(test_imgs_tensor.cpu()):\n",
    "    test_img = transform(test_img_tensor)\n",
    "    test_save_path = os.path.join(test_img_save_path, f\"{test_input[test_idx]}.jpg\")\n",
    "    test_img.save(Path(test_save_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Load DALLE2 Model (Use if you want to get the trained model)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\"\n",
    "\n",
    "# # openai pretrained clip - defaults to ViT/B-32\n",
    "# OpenAIClip = OpenAIClipAdapter()\n",
    "\n",
    "# prior_network = DiffusionPriorNetwork(\n",
    "#     dim = 512,\n",
    "#     depth = 6,\n",
    "#     dim_head = 64,\n",
    "#     heads = 8\n",
    "# )\n",
    "\n",
    "# diffusion_prior = DiffusionPrior(\n",
    "#     net = prior_network,\n",
    "#     clip = OpenAIClip,\n",
    "#     timesteps = 100,\n",
    "#     cond_drop_prob = 0.2\n",
    "# ).to(device)\n",
    "\n",
    "# # diffusion_prior = nn.DataParallel(diffusion_prior)\n",
    "\n",
    "# unet1 = Unet(\n",
    "#     dim = 128,\n",
    "#     image_embed_dim = 512,\n",
    "#     cond_dim = 128,\n",
    "#     channels = 3,\n",
    "#     dim_mults=(1, 2, 4, 8)\n",
    "# )\n",
    "\n",
    "# unet2 = Unet(\n",
    "#     dim = 16,\n",
    "#     image_embed_dim = 256,\n",
    "#     cond_dim = 128,\n",
    "#     channels = 3,\n",
    "#     dim_mults = (1, 2, 4, 8, 16)\n",
    "# )\n",
    "\n",
    "# decoder = Decoder(\n",
    "#     unet = (unet1, unet2),\n",
    "#     image_sizes = (128, 256),\n",
    "#     clip = OpenAIClip,\n",
    "#     timesteps = 100,\n",
    "#     image_cond_drop_prob = 0.1,\n",
    "#     text_cond_drop_prob = 0.5,\n",
    "#     condition_on_text_encodings = True  # set this to True if you wish to condition on text during training and sampling\n",
    "# ).to(device)\n",
    "\n",
    "# dalle2 = DALLE2(\n",
    "#     prior = diffusion_prior,\n",
    "#     decoder = decoder\n",
    "# ).cuda()\n",
    "\n",
    "# # Change your model path (\".pt\" file)\n",
    "# load_model_path = \"./dalle2.pt\"\n",
    "\n",
    "# dalle2.load_state_dict(torch.load(load_model_path))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
